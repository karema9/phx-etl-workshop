{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import task, flows\n",
    "import requests\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package prefect:\n",
      "\n",
      "NAME\n",
      "    prefect - # isort: skip_file\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _internal (package)\n",
      "    _vendor (package)\n",
      "    _version\n",
      "    agent\n",
      "    artifacts\n",
      "    blocks (package)\n",
      "    cli (package)\n",
      "    client (package)\n",
      "    concurrency (package)\n",
      "    context\n",
      "    deployments (package)\n",
      "    deprecated (package)\n",
      "    engine\n",
      "    events (package)\n",
      "    exceptions\n",
      "    filesystems\n",
      "    flows\n",
      "    futures\n",
      "    infrastructure (package)\n",
      "    logging (package)\n",
      "    manifests\n",
      "    packaging (package)\n",
      "    plugins\n",
      "    results\n",
      "    runner (package)\n",
      "    runtime (package)\n",
      "    serializers\n",
      "    server (package)\n",
      "    settings\n",
      "    software (package)\n",
      "    states\n",
      "    task_runners\n",
      "    tasks\n",
      "    testing (package)\n",
      "    utilities (package)\n",
      "    variables\n",
      "    workers (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        prefect.runner.runner.Runner\n",
      "    prefect._internal.schemas.bases.ObjectBaseModel(prefect._internal.schemas.bases.IDBaseModel)\n",
      "        prefect.client.schemas.objects.State(prefect._internal.schemas.bases.ObjectBaseModel, typing.Generic)\n",
      "    prefect.utilities.annotations.BaseAnnotation(prefect.utilities.annotations.BaseAnnotation, abc.ABC, typing.Generic)\n",
      "        prefect.utilities.annotations.allow_failure\n",
      "        prefect.utilities.annotations.unmapped\n",
      "    pydantic.v1.main.BaseModel(pydantic.v1.utils.Representation)\n",
      "        prefect.manifests.Manifest\n",
      "    typing.Generic(builtins.object)\n",
      "        prefect.client.schemas.objects.State(prefect._internal.schemas.bases.ObjectBaseModel, typing.Generic)\n",
      "        prefect.flows.Flow\n",
      "        prefect.tasks.Task\n",
      "    \n",
      "    class Flow(typing.Generic)\n",
      "     |  Flow(*args, **kwargs)\n",
      "     |  \n",
      "     |  A Prefect workflow definition.\n",
      "     |  \n",
      "     |  !!! note\n",
      "     |      We recommend using the [`@flow` decorator][prefect.flows.flow] for most use-cases.\n",
      "     |  \n",
      "     |  Wraps a function with an entrypoint to the Prefect engine. To preserve the input\n",
      "     |  and output types, we use the generic type variables `P` and `R` for \"Parameters\" and\n",
      "     |  \"Returns\" respectively.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      fn: The function defining the workflow.\n",
      "     |      name: An optional name for the flow; if not provided, the name will be inferred\n",
      "     |          from the given function.\n",
      "     |      version: An optional version string for the flow; if not provided, we will\n",
      "     |          attempt to create a version string as a hash of the file containing the\n",
      "     |          wrapped function; if the file cannot be located, the version will be null.\n",
      "     |      flow_run_name: An optional name to distinguish runs of this flow; this name can\n",
      "     |          be provided as a string template with the flow's parameters as variables,\n",
      "     |          or a function that returns a string.\n",
      "     |      task_runner: An optional task runner to use for task execution within the flow;\n",
      "     |          if not provided, a `ConcurrentTaskRunner` will be used.\n",
      "     |      description: An optional string description for the flow; if not provided, the\n",
      "     |          description will be pulled from the docstring for the decorated function.\n",
      "     |      timeout_seconds: An optional number of seconds indicating a maximum runtime for\n",
      "     |          the flow. If the flow exceeds this runtime, it will be marked as failed.\n",
      "     |          Flow execution may continue until the next task is called.\n",
      "     |      validate_parameters: By default, parameters passed to flows are validated by\n",
      "     |          Pydantic. This will check that input values conform to the annotated types\n",
      "     |          on the function. Where possible, values will be coerced into the correct\n",
      "     |          type; for example, if a parameter is defined as `x: int` and \"5\" is passed,\n",
      "     |          it will be resolved to `5`. If set to `False`, no validation will be\n",
      "     |          performed on flow parameters.\n",
      "     |      retries: An optional number of times to retry on flow run failure.\n",
      "     |      retry_delay_seconds: An optional number of seconds to wait before retrying the\n",
      "     |          flow after failure. This is only applicable if `retries` is nonzero.\n",
      "     |      persist_result: An optional toggle indicating whether the result of this flow\n",
      "     |          should be persisted to result storage. Defaults to `None`, which indicates\n",
      "     |          that Prefect should choose whether the result should be persisted depending on\n",
      "     |          the features being used.\n",
      "     |      result_storage: An optional block to use to persist the result of this flow.\n",
      "     |          This value will be used as the default for any tasks in this flow.\n",
      "     |          If not provided, the local file system will be used unless called as\n",
      "     |          a subflow, at which point the default will be loaded from the parent flow.\n",
      "     |      result_serializer: An optional serializer to use to serialize the result of this\n",
      "     |          flow for persistence. This value will be used as the default for any tasks\n",
      "     |          in this flow. If not provided, the value of `PREFECT_RESULTS_DEFAULT_SERIALIZER`\n",
      "     |          will be used unless called as a subflow, at which point the default will be\n",
      "     |          loaded from the parent flow.\n",
      "     |      on_failure: An optional list of callables to run when the flow enters a failed state.\n",
      "     |      on_completion: An optional list of callables to run when the flow enters a completed state.\n",
      "     |      on_cancellation: An optional list of callables to run when the flow enters a cancelling state.\n",
      "     |      on_crashed: An optional list of callables to run when the flow enters a crashed state.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Flow\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, *args: 'P.args', return_state: bool = False, wait_for: Optional[Iterable[prefect.futures.PrefectFuture]] = None, **kwargs: 'P.kwargs')\n",
      "     |      Run the flow and return its result.\n",
      "     |      \n",
      "     |      \n",
      "     |      Flow parameter values must be serializable by Pydantic.\n",
      "     |      \n",
      "     |      If writing an async flow, this call must be awaited.\n",
      "     |      \n",
      "     |      This will create a new flow run in the API.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          *args: Arguments to run the flow with.\n",
      "     |          return_state: Return a Prefect State containing the result of the\n",
      "     |              flow run.\n",
      "     |          wait_for: Upstream task futures to wait for before starting the flow if called as a subflow\n",
      "     |          **kwargs: Keyword arguments to run the flow with.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          If `return_state` is False, returns the result of the flow run.\n",
      "     |          If `return_state` is True, returns the result of the flow run\n",
      "     |              wrapped in a Prefect State which provides error handling.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |          Define a flow\n",
      "     |      \n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow(name):\n",
      "     |          >>>     print(f\"hello {name}\")\n",
      "     |          >>>     return f\"goodbye {name}\"\n",
      "     |      \n",
      "     |          Run a flow\n",
      "     |      \n",
      "     |          >>> my_flow(\"marvin\")\n",
      "     |          hello marvin\n",
      "     |          \"goodbye marvin\"\n",
      "     |      \n",
      "     |          Run a flow with additional tags\n",
      "     |      \n",
      "     |          >>> from prefect import tags\n",
      "     |          >>> with tags(\"db\", \"blue\"):\n",
      "     |          >>>     my_flow(\"foo\")\n",
      "     |  \n",
      "     |  __init__ = __register_init__(__self__, *args, **kwargs)\n",
      "     |  \n",
      "     |  deploy(self, name: str, work_pool_name: Optional[str] = None, image: Union[str, prefect.deployments.runner.DeploymentImage, NoneType] = None, build: bool = True, push: bool = True, work_queue_name: Optional[str] = None, job_variables: Optional[dict] = None, interval: Union[int, float, datetime.timedelta, NoneType] = None, cron: Optional[str] = None, rrule: Optional[str] = None, schedule: Union[prefect.client.schemas.schedules.IntervalSchedule, prefect.client.schemas.schedules.CronSchedule, prefect.client.schemas.schedules.RRuleSchedule, NoneType] = None, triggers: Optional[List[prefect.events.schemas.DeploymentTrigger]] = None, parameters: Optional[dict] = None, description: Optional[str] = None, tags: Optional[List[str]] = None, version: Optional[str] = None, enforce_parameter_schema: bool = False, print_next_steps: bool = True) -> uuid.UUID\n",
      "     |      Deploys a flow to run on dynamic infrastructure via a work pool.\n",
      "     |      \n",
      "     |      By default, calling this method will build a Docker image for the flow, push it to a registry,\n",
      "     |      and create a deployment via the Prefect API that will run the flow on the given schedule.\n",
      "     |      \n",
      "     |      If you want to use an existing image, you can pass `build=False` to skip building and pushing\n",
      "     |      an image.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name: The name to give the created deployment.\n",
      "     |          work_pool_name: The name of the work pool to use for this deployment. Defaults to\n",
      "     |              the value of `PREFECT_DEFAULT_WORK_POOL_NAME`.\n",
      "     |          image: The name of the Docker image to build, including the registry and\n",
      "     |              repository. Pass a DeploymentImage instance to customize the Dockerfile used\n",
      "     |              and build arguments.\n",
      "     |          build: Whether or not to build a new image for the flow. If False, the provided\n",
      "     |              image will be used as-is and pulled at runtime.\n",
      "     |          push: Whether or not to skip pushing the built image to a registry.\n",
      "     |          work_queue_name: The name of the work queue to use for this deployment's scheduled runs.\n",
      "     |              If not provided the default work queue for the work pool will be used.\n",
      "     |          job_variables: Settings used to override the values specified default base job template\n",
      "     |              of the chosen work pool. Refer to the base job template of the chosen work pool for\n",
      "     |              available settings.\n",
      "     |          interval: An interval on which to execute the new deployment. Accepts either a number\n",
      "     |              or a timedelta object. If a number is given, it will be interpreted as seconds.\n",
      "     |          cron: A cron schedule of when to execute runs of this deployment.\n",
      "     |          rrule: An rrule schedule of when to execute runs of this deployment.\n",
      "     |          triggers: A list of triggers that will kick off runs of this deployment.\n",
      "     |          schedule: A schedule object defining when to execute runs of this deployment. Used to\n",
      "     |              define additional scheduling options like `timezone`.\n",
      "     |          parameters: A dictionary of default parameter values to pass to runs of this deployment.\n",
      "     |          description: A description for the created deployment. Defaults to the flow's\n",
      "     |              description if not provided.\n",
      "     |          tags: A list of tags to associate with the created deployment for organizational\n",
      "     |              purposes.\n",
      "     |          version: A version for the created deployment. Defaults to the flow's version.\n",
      "     |          enforce_parameter_schema: Whether or not the Prefect API should enforce the\n",
      "     |              parameter schema for the created deployment.\n",
      "     |          print_next_steps_message: Whether or not to print a message with next steps\n",
      "     |          after deploying the deployments.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The ID of the created/updated deployment.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |          Deploy a local flow to a work pool:\n",
      "     |      \n",
      "     |          ```python\n",
      "     |          from prefect import flow\n",
      "     |      \n",
      "     |          @flow\n",
      "     |          def my_flow(name):\n",
      "     |              print(f\"hello {name}\")\n",
      "     |      \n",
      "     |          if __name__ == \"__main__\":\n",
      "     |              my_flow.deploy(\n",
      "     |                  \"example-deployment\",\n",
      "     |                  work_pool_name=\"my-work-pool\",\n",
      "     |                  image=\"my-repository/my-image:dev\",\n",
      "     |              )\n",
      "     |          ```\n",
      "     |      \n",
      "     |          Deploy a remotely stored flow to a work pool:\n",
      "     |      \n",
      "     |          ```python\n",
      "     |          from prefect import flow\n",
      "     |      \n",
      "     |          if __name__ == \"__main__\":\n",
      "     |              flow.from_source(\n",
      "     |                  source=\"https://github.com/org/repo.git\",\n",
      "     |                  entrypoint=\"flows.py:my_flow\",\n",
      "     |              ).deploy(\n",
      "     |                  \"example-deployment\",\n",
      "     |                  work_pool_name=\"my-work-pool\",\n",
      "     |                  image=\"my-repository/my-image:dev\",\n",
      "     |              )\n",
      "     |          ```\n",
      "     |  \n",
      "     |  serialize_parameters(self, parameters: Dict[str, Any]) -> Dict[str, Any]\n",
      "     |      Convert parameters to a serializable form.\n",
      "     |      \n",
      "     |      Uses FastAPI's `jsonable_encoder` to convert to JSON compatible objects without\n",
      "     |      converting everything directly to a string. This maintains basic types like\n",
      "     |      integers during API roundtrips.\n",
      "     |  \n",
      "     |  serve(self, name: str, interval: Union[int, float, datetime.timedelta, NoneType] = None, cron: Optional[str] = None, rrule: Optional[str] = None, schedule: Union[prefect.client.schemas.schedules.IntervalSchedule, prefect.client.schemas.schedules.CronSchedule, prefect.client.schemas.schedules.RRuleSchedule, NoneType] = None, triggers: Optional[List[prefect.events.schemas.DeploymentTrigger]] = None, parameters: Optional[dict] = None, description: Optional[str] = None, tags: Optional[List[str]] = None, version: Optional[str] = None, enforce_parameter_schema: bool = False, pause_on_shutdown: bool = True, print_starting_message: bool = True, webserver: bool = False)\n",
      "     |      Creates a deployment for this flow and starts a runner to monitor for scheduled work.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name: The name to give the created deployment.\n",
      "     |          interval: An interval on which to execute the new deployment. Accepts either a number\n",
      "     |              or a timedelta object. If a number is given, it will be interpreted as seconds.\n",
      "     |          cron: A cron schedule of when to execute runs of this deployment.\n",
      "     |          rrule: An rrule schedule of when to execute runs of this deployment.\n",
      "     |          triggers: A list of triggers that will kick off runs of this deployment.\n",
      "     |          schedule: A schedule object defining when to execute runs of this deployment. Used to\n",
      "     |              define additional scheduling options like `timezone`.\n",
      "     |          parameters: A dictionary of default parameter values to pass to runs of this deployment.\n",
      "     |          description: A description for the created deployment. Defaults to the flow's\n",
      "     |              description if not provided.\n",
      "     |          tags: A list of tags to associate with the created deployment for organizational\n",
      "     |              purposes.\n",
      "     |          version: A version for the created deployment. Defaults to the flow's version.\n",
      "     |          enforce_parameter_schema: Whether or not the Prefect API should enforce the\n",
      "     |              parameter schema for the created deployment.\n",
      "     |          pause_on_shutdown: If True, provided schedule will be paused when the serve function is stopped.\n",
      "     |              If False, the schedules will continue running.\n",
      "     |          print_starting_message: Whether or not to print the starting message when flow is served.\n",
      "     |          webserver: Whether or not to start a monitoring webserver for this flow.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |          Serve a flow:\n",
      "     |      \n",
      "     |          ```python\n",
      "     |          from prefect import flow\n",
      "     |      \n",
      "     |          @flow\n",
      "     |          def my_flow(name):\n",
      "     |              print(f\"hello {name}\")\n",
      "     |      \n",
      "     |          if __name__ == \"__main__\":\n",
      "     |              my_flow.serve(\"example-deployment\")\n",
      "     |          ```\n",
      "     |      \n",
      "     |          Serve a flow and run it every hour:\n",
      "     |      \n",
      "     |          ```python\n",
      "     |          from prefect import flow\n",
      "     |      \n",
      "     |          @flow\n",
      "     |          def my_flow(name):\n",
      "     |              print(f\"hello {name}\")\n",
      "     |      \n",
      "     |          if __name__ == \"__main__\":\n",
      "     |              my_flow.serve(\"example-deployment\", interval=3600)\n",
      "     |          ```\n",
      "     |  \n",
      "     |  to_deployment(self, name: str, interval: Union[int, float, datetime.timedelta, NoneType] = None, cron: Optional[str] = None, rrule: Optional[str] = None, schedule: Union[prefect.client.schemas.schedules.IntervalSchedule, prefect.client.schemas.schedules.CronSchedule, prefect.client.schemas.schedules.RRuleSchedule, NoneType] = None, parameters: Optional[dict] = None, triggers: Optional[List[prefect.events.schemas.DeploymentTrigger]] = None, description: Optional[str] = None, tags: Optional[List[str]] = None, version: Optional[str] = None, enforce_parameter_schema: bool = False, work_pool_name: Optional[str] = None, work_queue_name: Optional[str] = None, job_variables: Optional[Dict[str, Any]] = None)\n",
      "     |      Creates a runner deployment object for this flow.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name: The name to give the created deployment.\n",
      "     |          interval: An interval on which to execute the new deployment. Accepts either a number\n",
      "     |              or a timedelta object. If a number is given, it will be interpreted as seconds.\n",
      "     |          cron: A cron schedule of when to execute runs of this deployment.\n",
      "     |          rrule: An rrule schedule of when to execute runs of this deployment.\n",
      "     |          timezone: A timezone to use for the schedule. Defaults to UTC.\n",
      "     |          triggers: A list of triggers that will kick off runs of this deployment.\n",
      "     |          schedule: A schedule object defining when to execute runs of this deployment.\n",
      "     |          parameters: A dictionary of default parameter values to pass to runs of this deployment.\n",
      "     |          description: A description for the created deployment. Defaults to the flow's\n",
      "     |              description if not provided.\n",
      "     |          tags: A list of tags to associate with the created deployment for organizational\n",
      "     |              purposes.\n",
      "     |          version: A version for the created deployment. Defaults to the flow's version.\n",
      "     |          enforce_parameter_schema: Whether or not the Prefect API should enforce the\n",
      "     |              parameter schema for the created deployment.\n",
      "     |          work_pool_name: The name of the work pool to use for this deployment.\n",
      "     |          work_queue_name: The name of the work queue to use for this deployment's scheduled runs.\n",
      "     |              If not provided the default work queue for the work pool will be used.\n",
      "     |          job_variables: Settings used to override the values specified default base job template\n",
      "     |              of the chosen work pool. Refer to the base job template of the chosen work pool for\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |          Prepare two deployments and serve them:\n",
      "     |      \n",
      "     |          ```python\n",
      "     |          from prefect import flow, serve\n",
      "     |      \n",
      "     |          @flow\n",
      "     |          def my_flow(name):\n",
      "     |              print(f\"hello {name}\")\n",
      "     |      \n",
      "     |          @flow\n",
      "     |          def my_other_flow(name):\n",
      "     |              print(f\"goodbye {name}\")\n",
      "     |      \n",
      "     |          if __name__ == \"__main__\":\n",
      "     |              hello_deploy = my_flow.to_deployment(\"hello\", tags=[\"dev\"])\n",
      "     |              bye_deploy = my_other_flow.to_deployment(\"goodbye\", tags=[\"dev\"])\n",
      "     |              serve(hello_deploy, bye_deploy)\n",
      "     |          ```\n",
      "     |  \n",
      "     |  validate_parameters(self, parameters: Dict[str, Any]) -> Dict[str, Any]\n",
      "     |      Validate parameters for compatibility with the flow by attempting to cast the inputs to the\n",
      "     |      associated types specified by the function's type annotations.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A new dict of parameters that have been cast to the appropriate types\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ParameterTypeError: if the provided parameters are not valid\n",
      "     |  \n",
      "     |  visualize(self, *args, **kwargs)\n",
      "     |      Generates a graphviz object representing the current flow. In IPython notebooks,\n",
      "     |      it's rendered inline, otherwise in a new window as a PNG.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          - ImportError: If `graphviz` isn't installed.\n",
      "     |          - GraphvizExecutableNotFoundError: If the `dot` executable isn't found.\n",
      "     |          - FlowVisualizationError: If the flow can't be visualized for any other reason.\n",
      "     |  \n",
      "     |  with_options(self, *, name: str = None, version: str = None, retries: int = 0, retry_delay_seconds: Union[int, float] = 0, description: str = None, flow_run_name: Union[Callable[[], str], str, NoneType] = None, task_runner: Union[Type[prefect.task_runners.BaseTaskRunner], prefect.task_runners.BaseTaskRunner] = None, timeout_seconds: Union[int, float] = None, validate_parameters: bool = None, persist_result: Optional[bool] = <class 'prefect.utilities.annotations.NotSet'>, result_storage: Union[prefect.filesystems.WritableFileSystem, str, NoneType] = <class 'prefect.utilities.annotations.NotSet'>, result_serializer: Union[prefect.serializers.Serializer, str, NoneType] = <class 'prefect.utilities.annotations.NotSet'>, cache_result_in_memory: bool = None, log_prints: Optional[bool] = <class 'prefect.utilities.annotations.NotSet'>, on_completion: Optional[List[Callable[[prefect.client.schemas.objects.Flow, prefect.client.schemas.objects.FlowRun, prefect.client.schemas.objects.State], NoneType]]] = None, on_failure: Optional[List[Callable[[prefect.client.schemas.objects.Flow, prefect.client.schemas.objects.FlowRun, prefect.client.schemas.objects.State], NoneType]]] = None, on_cancellation: Optional[List[Callable[[prefect.client.schemas.objects.Flow, prefect.client.schemas.objects.FlowRun, prefect.client.schemas.objects.State], NoneType]]] = None, on_crashed: Optional[List[Callable[[prefect.client.schemas.objects.Flow, prefect.client.schemas.objects.FlowRun, prefect.client.schemas.objects.State], NoneType]]] = None)\n",
      "     |      Create a new flow from the current object, updating provided options.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name: A new name for the flow.\n",
      "     |          version: A new version for the flow.\n",
      "     |          description: A new description for the flow.\n",
      "     |          flow_run_name: An optional name to distinguish runs of this flow; this name\n",
      "     |              can be provided as a string template with the flow's parameters as variables,\n",
      "     |              or a function that returns a string.\n",
      "     |          task_runner: A new task runner for the flow.\n",
      "     |          timeout_seconds: A new number of seconds to fail the flow after if still\n",
      "     |              running.\n",
      "     |          validate_parameters: A new value indicating if flow calls should validate\n",
      "     |              given parameters.\n",
      "     |          retries: A new number of times to retry on flow run failure.\n",
      "     |          retry_delay_seconds: A new number of seconds to wait before retrying the\n",
      "     |              flow after failure. This is only applicable if `retries` is nonzero.\n",
      "     |          persist_result: A new option for enabling or disabling result persistence.\n",
      "     |          result_storage: A new storage type to use for results.\n",
      "     |          result_serializer: A new serializer to use for results.\n",
      "     |          cache_result_in_memory: A new value indicating if the flow's result should\n",
      "     |              be cached in memory.\n",
      "     |          on_failure: A new list of callables to run when the flow enters a failed state.\n",
      "     |          on_completion: A new list of callables to run when the flow enters a completed state.\n",
      "     |          on_cancellation: A new list of callables to run when the flow enters a cancelling state.\n",
      "     |          on_crashed: A new list of callables to run when the flow enters a crashed state.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A new `Flow` instance.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |          Create a new flow from an existing flow and update the name:\n",
      "     |      \n",
      "     |          >>> @flow(name=\"My flow\")\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     return 1\n",
      "     |          >>>\n",
      "     |          >>> new_flow = my_flow.with_options(name=\"My new flow\")\n",
      "     |      \n",
      "     |          Create a new flow from an existing flow, update the task runner, and call\n",
      "     |          it without an intermediate variable:\n",
      "     |      \n",
      "     |          >>> from prefect.task_runners import SequentialTaskRunner\n",
      "     |          >>>\n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow(x, y):\n",
      "     |          >>>     return x + y\n",
      "     |          >>>\n",
      "     |          >>> state = my_flow.with_options(task_runner=SequentialTaskRunner)(1, 3)\n",
      "     |          >>> assert state.result() == 4\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_source(source: Union[str, prefect.runner.storage.RunnerStorage, prefect.filesystems.ReadableDeploymentStorage], entrypoint: str) -> 'Flow' from builtins.type\n",
      "     |      Loads a flow from a remote s ource.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          source: Either a URL to a git repository or a storage object.\n",
      "     |          entrypoint:  The path to a file containing a flow and the name of the flow function in\n",
      "     |              the format `./path/to/file.py:flow_func_name`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A new `Flow` instance.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |          Load a flow from a public git repository:\n",
      "     |      \n",
      "     |      \n",
      "     |          ```python\n",
      "     |          from prefect import flow\n",
      "     |          from prefect.runner.storage import GitRepository\n",
      "     |          from prefect.blocks.system import Secret\n",
      "     |      \n",
      "     |          my_flow = flow.from_source(\n",
      "     |              source=\"https://github.com/org/repo.git\",\n",
      "     |              entrypoint=\"flows.py:my_flow\",\n",
      "     |          )\n",
      "     |      \n",
      "     |          my_flow()\n",
      "     |          ```\n",
      "     |      \n",
      "     |          Load a flow from a private git repository:\n",
      "     |      \n",
      "     |          ```python\n",
      "     |          from prefect import flow\n",
      "     |          from prefect.runner.storage import GitRepository\n",
      "     |          from prefect.blocks.system import Secret\n",
      "     |      \n",
      "     |          my_flow = flow.from_source(\n",
      "     |              source=GitRepository(\n",
      "     |                  url=\"https://github.com/org/repo.git\",\n",
      "     |                  access_token=Secret.load(\"github-access-token\").get(),\n",
      "     |              ),\n",
      "     |              entrypoint=\"flows.py:my_flow\",\n",
      "     |          )\n",
      "     |      \n",
      "     |          my_flow()\n",
      "     |          ```\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[~P, ~R],)\n",
      "     |  \n",
      "     |  __parameters__ = (~P, ~R)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Manifest(pydantic.v1.main.BaseModel)\n",
      "     |  Manifest(*, flow_name: str, import_path: str, parameter_openapi_schema: prefect.utilities.callables.ParameterSchema) -> None\n",
      "     |  \n",
      "     |  A JSON representation of a flow.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Manifest\n",
      "     |      pydantic.v1.main.BaseModel\n",
      "     |      pydantic.v1.utils.Representation\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'flow_name': <class 'str'>, 'import_path': <class '...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = None\n",
      "     |  \n",
      "     |  __fields__ = {'flow_name': ModelField(name='flow_name', type=str, requ...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __post_root_validators__ = []\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {}\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, flow_name: str, import_path: str,...ect...\n",
      "     |  \n",
      "     |  __validators__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __init__(__pydantic_self__, **data: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      "     |      \n",
      "     |      Can either return:\n",
      "     |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      "     |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -> 'DictStrAny'\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.v1.main.BaseModel:\n",
      "     |  \n",
      "     |  Config = <class 'pydantic.v1.config.BaseConfig'>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.v1.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __repr_name__(self) -> str\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: str) -> str\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  __str__(self) -> str\n",
      "     |      Return str(self).\n",
      "    \n",
      "    class Runner(builtins.object)\n",
      "     |  Runner(name: Optional[str] = None, query_seconds: Optional[float] = None, prefetch_seconds: float = 10, limit: Optional[int] = None, pause_on_shutdown: bool = True, webserver: bool = False)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  async __aenter__(self)\n",
      "     |  \n",
      "     |  async __aexit__(self, *exc_info)\n",
      "     |  \n",
      "     |  __init__(self, name: Optional[str] = None, query_seconds: Optional[float] = None, prefetch_seconds: float = 10, limit: Optional[int] = None, pause_on_shutdown: bool = True, webserver: bool = False)\n",
      "     |      Responsible for managing the execution of remotely initiated flow runs.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name: The name of the runner. If not provided, a random one\n",
      "     |              will be generated. If provided, it cannot contain '/' or '%'.\n",
      "     |          query_seconds: The number of seconds to wait between querying for\n",
      "     |              scheduled flow runs; defaults to `PREFECT_RUNNER_POLL_FREQUENCY`\n",
      "     |          prefetch_seconds: The number of seconds to prefetch flow runs for.\n",
      "     |          limit: The maximum number of flow runs this runner should be running at\n",
      "     |          pause_on_shutdown: A boolean for whether or not to automatically pause\n",
      "     |              deployment schedules on shutdown; defaults to `True`\n",
      "     |          webserver: a boolean flag for whether to start a webserver for this runner\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |          Set up a Runner to manage the execute of scheduled flow runs for two flows:\n",
      "     |              ```python\n",
      "     |              from prefect import flow, Runner\n",
      "     |      \n",
      "     |              @flow\n",
      "     |              def hello_flow(name):\n",
      "     |                  print(f\"hello {name}\")\n",
      "     |      \n",
      "     |              @flow\n",
      "     |              def goodbye_flow(name):\n",
      "     |                  print(f\"goodbye {name}\")\n",
      "     |      \n",
      "     |              if __name__ == \"__main__\"\n",
      "     |                  runner = Runner(name=\"my-runner\")\n",
      "     |      \n",
      "     |                  # Will be runnable via the API\n",
      "     |                  runner.add_flow(hello_flow)\n",
      "     |      \n",
      "     |                  # Run on a cron schedule\n",
      "     |                  runner.add_flow(goodbye_flow, schedule={\"cron\": \"0 * * * *\"})\n",
      "     |      \n",
      "     |                  runner.start()\n",
      "     |              ```\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  add_deployment(self, deployment: prefect.deployments.runner.RunnerDeployment) -> uuid.UUID\n",
      "     |      Registers the deployment with the Prefect API and will monitor for work once\n",
      "     |      the runner is started.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          deployment: A deployment for the runner to register.\n",
      "     |  \n",
      "     |  add_flow(self, flow: prefect.flows.Flow, name: str = None, interval: Union[int, float, datetime.timedelta, NoneType] = None, cron: Optional[str] = None, rrule: Optional[str] = None, schedule: Union[prefect.client.schemas.schedules.IntervalSchedule, prefect.client.schemas.schedules.CronSchedule, prefect.client.schemas.schedules.RRuleSchedule, NoneType] = None, parameters: Optional[dict] = None, triggers: Optional[List[prefect.events.schemas.DeploymentTrigger]] = None, description: Optional[str] = None, tags: Optional[List[str]] = None, version: Optional[str] = None, enforce_parameter_schema: bool = False) -> uuid.UUID\n",
      "     |      Provides a flow to the runner to be run based on the provided configuration.\n",
      "     |      \n",
      "     |      Will create a deployment for the provided flow and register the deployment\n",
      "     |      with the runner.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          flow: A flow for the runner to run.\n",
      "     |          name: The name to give the created deployment. Will default to the name\n",
      "     |              of the runner.\n",
      "     |          interval: An interval on which to execute the current flow. Accepts either a number\n",
      "     |              or a timedelta object. If a number is given, it will be interpreted as seconds.\n",
      "     |          cron: A cron schedule of when to execute runs of this flow.\n",
      "     |          rrule: An rrule schedule of when to execute runs of this flow.\n",
      "     |          schedule: A schedule object of when to execute runs of this flow. Used for\n",
      "     |              advanced scheduling options like timezone.\n",
      "     |          triggers: A list of triggers that should kick of a run of this flow.\n",
      "     |          parameters: A dictionary of default parameter values to pass to runs of this flow.\n",
      "     |          description: A description for the created deployment. Defaults to the flow's\n",
      "     |              description if not provided.\n",
      "     |          tags: A list of tags to associate with the created deployment for organizational\n",
      "     |              purposes.\n",
      "     |          version: A version for the created deployment. Defaults to the flow's version.\n",
      "     |  \n",
      "     |  async cancel_all(self)\n",
      "     |  \n",
      "     |  async execute_flow_run(self, flow_run_id: uuid.UUID)\n",
      "     |      Executes a single flow run with the given ID.\n",
      "     |      \n",
      "     |      Execution will wait to monitor for cancellation requests. Exits once\n",
      "     |      the flow run process has exited.\n",
      "     |  \n",
      "     |  handle_sigterm(self, signum, frame)\n",
      "     |      Gracefully shuts down the runner when a SIGTERM is received.\n",
      "     |  \n",
      "     |  start(self, run_once: bool = False, webserver: Optional[bool] = None) -> None\n",
      "     |      Starts a runner.\n",
      "     |      \n",
      "     |      The runner will begin monitoring for and executing any scheduled work for all added flows.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          run_once: If True, the runner will through one query loop and then exit.\n",
      "     |          webserver: a boolean for whether to start a webserver for this runner. If provided,\n",
      "     |              overrides the default on the runner\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |          Initialize a Runner, add two flows, and serve them by starting the Runner:\n",
      "     |      \n",
      "     |          ```python\n",
      "     |          from prefect import flow, Runner\n",
      "     |      \n",
      "     |          @flow\n",
      "     |          def hello_flow(name):\n",
      "     |              print(f\"hello {name}\")\n",
      "     |      \n",
      "     |          @flow\n",
      "     |          def goodbye_flow(name):\n",
      "     |              print(f\"goodbye {name}\")\n",
      "     |      \n",
      "     |          if __name__ == \"__main__\"\n",
      "     |              runner = Runner(name=\"my-runner\")\n",
      "     |      \n",
      "     |              # Will be runnable via the API\n",
      "     |              runner.add_flow(hello_flow)\n",
      "     |      \n",
      "     |              # Run on a cron schedule\n",
      "     |              runner.add_flow(goodbye_flow, schedule={\"cron\": \"0 * * * *\"})\n",
      "     |      \n",
      "     |              runner.start()\n",
      "     |          ```\n",
      "     |  \n",
      "     |  stop(self)\n",
      "     |      Stops the runner's polling cycle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class State(prefect._internal.schemas.bases.ObjectBaseModel, typing.Generic)\n",
      "     |  State(*, id: uuid.UUID = None, created: Optional[prefect._internal.schemas.fields.DateTimeTZ] = None, updated: Optional[prefect._internal.schemas.fields.DateTimeTZ] = None, type: prefect.client.schemas.objects.StateType, name: Optional[str] = None, timestamp: prefect._internal.schemas.fields.DateTimeTZ = None, message: Optional[str] = None, state_details: prefect.client.schemas.objects.StateDetails = None, data: Union[ForwardRef('BaseResult[R]'), ForwardRef('DataDocument[R]'), Any] = None) -> None\n",
      "     |  \n",
      "     |  The state of a run.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      State\n",
      "     |      prefect._internal.schemas.bases.ObjectBaseModel\n",
      "     |      prefect._internal.schemas.bases.IDBaseModel\n",
      "     |      prefect._internal.schemas.bases.PrefectBaseModel\n",
      "     |      pydantic.v1.main.BaseModel\n",
      "     |      pydantic.v1.utils.Representation\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __hash__(self) -> int\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Generates a complete state representation appropriate for introspection\n",
      "     |      and debugging, including the result:\n",
      "     |      \n",
      "     |      `MyCompletedState(message=\"my message\", type=COMPLETED, result=...)`\n",
      "     |  \n",
      "     |  __str__(self) -> str\n",
      "     |      Generates a simple state representation appropriate for logging:\n",
      "     |      \n",
      "     |      `MyCompletedState(\"my message\", type=COMPLETED)`\n",
      "     |  \n",
      "     |  copy(self, *, update: dict = None, reset_fields: bool = False, **kwargs)\n",
      "     |      Copying API models should return an object that could be inserted into the\n",
      "     |      database again. The 'timestamp' is reset using the default factory.\n",
      "     |  \n",
      "     |  is_cancelled(self) -> bool\n",
      "     |  \n",
      "     |  is_cancelling(self) -> bool\n",
      "     |  \n",
      "     |  is_completed(self) -> bool\n",
      "     |  \n",
      "     |  is_crashed(self) -> bool\n",
      "     |  \n",
      "     |  is_failed(self) -> bool\n",
      "     |  \n",
      "     |  is_final(self) -> bool\n",
      "     |  \n",
      "     |  is_paused(self) -> bool\n",
      "     |  \n",
      "     |  is_pending(self) -> bool\n",
      "     |  \n",
      "     |  is_running(self) -> bool\n",
      "     |  \n",
      "     |  is_scheduled(self) -> bool\n",
      "     |  \n",
      "     |  result(self, raise_on_failure: bool = True, fetch: Optional[bool] = None)\n",
      "     |      Retrieve the result attached to this state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          raise_on_failure: a boolean specifying whether to raise an exception\n",
      "     |              if the state is of type `FAILED` and the underlying data is an exception\n",
      "     |          fetch: a boolean specifying whether to resolve references to persisted\n",
      "     |              results into data. For synchronous users, this defaults to `True`.\n",
      "     |              For asynchronous users, this defaults to `False` for backwards\n",
      "     |              compatibility.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          TypeError: If the state is failed but the result is not an exception.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the run\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |          >>> from prefect import flow, task\n",
      "     |          >>> @task\n",
      "     |          >>> def my_task(x):\n",
      "     |          >>>     return x\n",
      "     |      \n",
      "     |          Get the result from a task future in a flow\n",
      "     |      \n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     future = my_task(\"hello\")\n",
      "     |          >>>     state = future.wait()\n",
      "     |          >>>     result = state.result()\n",
      "     |          >>>     print(result)\n",
      "     |          >>> my_flow()\n",
      "     |          hello\n",
      "     |      \n",
      "     |          Get the result from a flow state\n",
      "     |      \n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     return \"hello\"\n",
      "     |          >>> my_flow(return_state=True).result()\n",
      "     |          hello\n",
      "     |      \n",
      "     |          Get the result from a failed state\n",
      "     |      \n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     raise ValueError(\"oh no!\")\n",
      "     |          >>> state = my_flow(return_state=True)  # Error is wrapped in FAILED state\n",
      "     |          >>> state.result()  # Raises `ValueError`\n",
      "     |      \n",
      "     |          Get the result from a failed state without erroring\n",
      "     |      \n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     raise ValueError(\"oh no!\")\n",
      "     |          >>> state = my_flow(return_state=True)\n",
      "     |          >>> result = state.result(raise_on_failure=False)\n",
      "     |          >>> print(result)\n",
      "     |          ValueError(\"oh no!\")\n",
      "     |      \n",
      "     |      \n",
      "     |          Get the result from a flow state in an async context\n",
      "     |      \n",
      "     |          >>> @flow\n",
      "     |          >>> async def my_flow():\n",
      "     |          >>>     return \"hello\"\n",
      "     |          >>> state = await my_flow(return_state=True)\n",
      "     |          >>> await state.result()\n",
      "     |          hello\n",
      "     |  \n",
      "     |  to_state_create(self)\n",
      "     |      Convert this state to a `StateCreate` type which can be used to set the state of\n",
      "     |      a run in the API.\n",
      "     |      \n",
      "     |      This method will drop this state's `data` if it is not a result type. Only\n",
      "     |      results should be sent to the API. Other data is only available locally.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  default_name_from_type(v, *, values, **kwargs) from pydantic.v1.main.ModelMetaclass\n",
      "     |      If a name is not provided, use the type\n",
      "     |  \n",
      "     |  default_scheduled_start_time(values) from pydantic.v1.main.ModelMetaclass\n",
      "     |      TODO: This should throw an error instead of setting a default but is out of\n",
      "     |            scope for https://github.com/PrefectHQ/orion/pull/174/ and can be rolled\n",
      "     |            into work refactoring state initialization\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = functools.partial(<function custom_pydantic_enco...ectBaseModel.Config.<lambda> at 0x7fb612ca8310>})\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'data': typing.Union[ForwardRef('BaseResult[R]'), F...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = None\n",
      "     |  \n",
      "     |  __fields__ = {'created': ModelField(name='created', type=Optional[Date...\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __orig_bases__ = (<class 'prefect._internal.schemas.bases.ObjectBaseMo...\n",
      "     |  \n",
      "     |  __parameters__ = (~R,)\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function State.default_scheduled_...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {}\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, id: uuid.UUID = None, created: Op...war...\n",
      "     |  \n",
      "     |  __validators__ = {'name': [<pydantic.v1.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from prefect._internal.schemas.bases.ObjectBaseModel:\n",
      "     |  \n",
      "     |  Config = <class 'prefect._internal.schemas.bases.ObjectBaseModel.Confi...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from prefect._internal.schemas.bases.PrefectBaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |      Equaltiy operator that ignores the resettable fields of the PrefectBaseModel.\n",
      "     |      \n",
      "     |      NOTE: this equality operator will only be applied if the PrefectBaseModel is\n",
      "     |      the left-hand operand. This is a limitation of Python.\n",
      "     |  \n",
      "     |  __rich_repr__(self)\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  dict(self, *args, shallow: bool = False, json_compatible: bool = False, **kwargs) -> dict\n",
      "     |      Returns a representation of the model as a Python dictionary.\n",
      "     |      \n",
      "     |      For more information on this distinction please see\n",
      "     |      https://pydantic-docs.helpmanual.io/usage/exporting_models/#dictmodel-and-iteration\n",
      "     |      \n",
      "     |      \n",
      "     |      Args:\n",
      "     |          shallow (bool, optional): If True (default), nested Pydantic fields\n",
      "     |              are also coerced to dicts. If false, they are left as Pydantic\n",
      "     |              models.\n",
      "     |          json_compatible (bool, optional): if True, objects are converted\n",
      "     |              into json-compatible representations, similar to calling\n",
      "     |              `json.loads(self.json())`. Not compatible with shallow=True.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict\n",
      "     |  \n",
      "     |  json(self, *args, include_secrets: bool = False, **kwargs) -> str\n",
      "     |      Returns a representation of the model as JSON.\n",
      "     |      \n",
      "     |      If `include_secrets=True`, then `SecretStr` and `SecretBytes` objects are\n",
      "     |      fully revealed. Otherwise they are obfuscated.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __init__(__pydantic_self__, **data: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      "     |      \n",
      "     |      Can either return:\n",
      "     |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      "     |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.v1.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr_name__(self) -> str\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: str) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.v1.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.v1.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Task(typing.Generic)\n",
      "     |  Task(*args, **kwargs)\n",
      "     |  \n",
      "     |  A Prefect task definition.\n",
      "     |  \n",
      "     |  !!! note\n",
      "     |      We recommend using [the `@task` decorator][prefect.tasks.task] for most use-cases.\n",
      "     |  \n",
      "     |  Wraps a function with an entrypoint to the Prefect engine. Calling this class within a flow function\n",
      "     |  creates a new task run.\n",
      "     |  \n",
      "     |  To preserve the input and output types, we use the generic type variables P and R for \"Parameters\" and\n",
      "     |  \"Returns\" respectively.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      fn: The function defining the task.\n",
      "     |      name: An optional name for the task; if not provided, the name will be inferred\n",
      "     |          from the given function.\n",
      "     |      description: An optional string description for the task.\n",
      "     |      tags: An optional set of tags to be associated with runs of this task. These\n",
      "     |          tags are combined with any tags defined by a `prefect.tags` context at\n",
      "     |          task runtime.\n",
      "     |      version: An optional string specifying the version of this task definition\n",
      "     |      cache_key_fn: An optional callable that, given the task run context and call\n",
      "     |          parameters, generates a string key; if the key matches a previous completed\n",
      "     |          state, that state result will be restored instead of running the task again.\n",
      "     |      cache_expiration: An optional amount of time indicating how long cached states\n",
      "     |          for this task should be restorable; if not provided, cached states will\n",
      "     |          never expire.\n",
      "     |      task_run_name: An optional name to distinguish runs of this task; this name can be provided\n",
      "     |          as a string template with the task's keyword arguments as variables,\n",
      "     |          or a function that returns a string.\n",
      "     |      retries: An optional number of times to retry on task run failure.\n",
      "     |      retry_delay_seconds: Optionally configures how long to wait before retrying the\n",
      "     |          task after failure. This is only applicable if `retries` is nonzero. This\n",
      "     |          setting can either be a number of seconds, a list of retry delays, or a\n",
      "     |          callable that, given the total number of retries, generates a list of retry\n",
      "     |          delays. If a number of seconds, that delay will be applied to all retries.\n",
      "     |          If a list, each retry will wait for the corresponding delay before retrying.\n",
      "     |          When passing a callable or a list, the number of configured retry delays\n",
      "     |          cannot exceed 50.\n",
      "     |      retry_jitter_factor: An optional factor that defines the factor to which a retry\n",
      "     |          can be jittered in order to avoid a \"thundering herd\".\n",
      "     |      persist_result: An optional toggle indicating whether the result of this task\n",
      "     |          should be persisted to result storage. Defaults to `None`, which indicates\n",
      "     |          that Prefect should choose whether the result should be persisted depending on\n",
      "     |          the features being used.\n",
      "     |      result_storage: An optional block to use to persist the result of this task.\n",
      "     |          Defaults to the value set in the flow the task is called in.\n",
      "     |      result_storage_key: An optional key to store the result in storage at when persisted.\n",
      "     |          Defaults to a unique identifier.\n",
      "     |      result_serializer: An optional serializer to use to serialize the result of this\n",
      "     |          task for persistence. Defaults to the value set in the flow the task is\n",
      "     |          called in.\n",
      "     |      timeout_seconds: An optional number of seconds indicating a maximum runtime for\n",
      "     |          the task. If the task exceeds this runtime, it will be marked as failed.\n",
      "     |      log_prints: If set, `print` statements in the task will be redirected to the\n",
      "     |          Prefect logger for the task run. Defaults to `None`, which indicates\n",
      "     |          that the value from the flow should be used.\n",
      "     |      refresh_cache: If set, cached results for the cache key are not used.\n",
      "     |          Defaults to `None`, which indicates that a cached result from a previous\n",
      "     |          execution with matching cache key is used.\n",
      "     |      on_failure: An optional list of callables to run when the task enters a failed state.\n",
      "     |      on_completion: An optional list of callables to run when the task enters a completed state.\n",
      "     |      viz_return_value: An optional value to return when the task dependency tree is visualized.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Task\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, *args: P.args, return_state: bool = False, wait_for: Optional[Iterable[prefect.futures.PrefectFuture]] = None, **kwargs: P.kwargs)\n",
      "     |      Run the task and return the result. If `return_state` is True returns\n",
      "     |      the result is wrapped in a Prefect State which provides error handling.\n",
      "     |  \n",
      "     |  __init__ = __register_init__(__self__, *args, **kwargs)\n",
      "     |  \n",
      "     |  map(self, *args: Any, return_state: bool = False, wait_for: Optional[Iterable[prefect.futures.PrefectFuture]] = None, **kwargs: Any) -> Any\n",
      "     |      Submit a mapped run of the task to a worker.\n",
      "     |      \n",
      "     |      Must be called within a flow function. If writing an async task, this\n",
      "     |      call must be awaited.\n",
      "     |      \n",
      "     |      Must be called with at least one iterable and all iterables must be\n",
      "     |      the same length. Any arguments that are not iterable will be treated as\n",
      "     |      a static value and each task run will receive the same value.\n",
      "     |      \n",
      "     |      Will create as many task runs as the length of the iterable(s) in the\n",
      "     |      backing API and submit the task runs to the flow's task runner. This\n",
      "     |      call blocks if given a future as input while the future is resolved. It\n",
      "     |      also blocks while the tasks are being submitted, once they are\n",
      "     |      submitted, the flow function will continue executing. However, note\n",
      "     |      that the `SequentialTaskRunner` does not implement parallel execution\n",
      "     |      for sync tasks and they are fully resolved on submission.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          *args: Iterable and static arguments to run the tasks with\n",
      "     |          return_state: Return a list of Prefect States that wrap the results\n",
      "     |              of each task run.\n",
      "     |          wait_for: Upstream task futures to wait for before starting the\n",
      "     |              task\n",
      "     |          **kwargs: Keyword iterable arguments to run the task with\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of futures allowing asynchronous access to the state of the\n",
      "     |          tasks\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |          Define a task\n",
      "     |      \n",
      "     |          >>> from prefect import task\n",
      "     |          >>> @task\n",
      "     |          >>> def my_task(x):\n",
      "     |          >>>     return x + 1\n",
      "     |      \n",
      "     |          Create mapped tasks\n",
      "     |      \n",
      "     |          >>> from prefect import flow\n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     my_task.map([1, 2, 3])\n",
      "     |      \n",
      "     |          Wait for all mapped tasks to finish\n",
      "     |      \n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     futures = my_task.map([1, 2, 3])\n",
      "     |          >>>     for future in futures:\n",
      "     |          >>>         future.wait()\n",
      "     |          >>>     # Now all of the mapped tasks have finished\n",
      "     |          >>>     my_task(10)\n",
      "     |      \n",
      "     |          Use the result from mapped tasks in a flow\n",
      "     |      \n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     futures = my_task.map([1, 2, 3])\n",
      "     |          >>>     for future in futures:\n",
      "     |          >>>         print(future.result())\n",
      "     |          >>> my_flow()\n",
      "     |          2\n",
      "     |          3\n",
      "     |          4\n",
      "     |      \n",
      "     |          Enforce ordering between tasks that do not exchange data\n",
      "     |          >>> @task\n",
      "     |          >>> def task_1(x):\n",
      "     |          >>>     pass\n",
      "     |          >>>\n",
      "     |          >>> @task\n",
      "     |          >>> def task_2(y):\n",
      "     |          >>>     pass\n",
      "     |          >>>\n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     x = task_1.submit()\n",
      "     |          >>>\n",
      "     |          >>>     # task 2 will wait for task_1 to complete\n",
      "     |          >>>     y = task_2.map([1, 2, 3], wait_for=[x])\n",
      "     |      \n",
      "     |          Use a non-iterable input as a constant across mapped tasks\n",
      "     |          >>> @task\n",
      "     |          >>> def display(prefix, item):\n",
      "     |          >>>    print(prefix, item)\n",
      "     |          >>>\n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     display.map(\"Check it out: \", [1, 2, 3])\n",
      "     |          >>>\n",
      "     |          >>> my_flow()\n",
      "     |          Check it out: 1\n",
      "     |          Check it out: 2\n",
      "     |          Check it out: 3\n",
      "     |      \n",
      "     |          Use `unmapped` to treat an iterable argument as a constant\n",
      "     |          >>> from prefect import unmapped\n",
      "     |          >>>\n",
      "     |          >>> @task\n",
      "     |          >>> def add_n_to_items(items, n):\n",
      "     |          >>>     return [item + n for item in items]\n",
      "     |          >>>\n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     return add_n_to_items.map(unmapped([10, 20]), n=[1, 2, 3])\n",
      "     |          >>>\n",
      "     |          >>> my_flow()\n",
      "     |          [[11, 21], [12, 22], [13, 23]]\n",
      "     |  \n",
      "     |  submit(self, *args: Any, return_state: bool = False, wait_for: Optional[Iterable[prefect.futures.PrefectFuture]] = None, **kwargs: Any) -> Union[prefect.futures.PrefectFuture, Awaitable[prefect.futures.PrefectFuture]]\n",
      "     |      Submit a run of the task to a worker.\n",
      "     |      \n",
      "     |      Must be called within a flow function. If writing an async task, this call must\n",
      "     |      be awaited.\n",
      "     |      \n",
      "     |      Will create a new task run in the backing API and submit the task to the flow's\n",
      "     |      task runner. This call only blocks execution while the task is being submitted,\n",
      "     |      once it is submitted, the flow function will continue executing. However, note\n",
      "     |      that the `SequentialTaskRunner` does not implement parallel execution for sync tasks\n",
      "     |      and they are fully resolved on submission.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          *args: Arguments to run the task with\n",
      "     |          return_state: Return the result of the flow run wrapped in a\n",
      "     |              Prefect State.\n",
      "     |          wait_for: Upstream task futures to wait for before starting the task\n",
      "     |          **kwargs: Keyword arguments to run the task with\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          If `return_state` is False a future allowing asynchronous access to\n",
      "     |              the state of the task\n",
      "     |          If `return_state` is True a future wrapped in a Prefect State allowing asynchronous access to\n",
      "     |              the state of the task\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |          Define a task\n",
      "     |      \n",
      "     |          >>> from prefect import task\n",
      "     |          >>> @task\n",
      "     |          >>> def my_task():\n",
      "     |          >>>     return \"hello\"\n",
      "     |      \n",
      "     |          Run a task in a flow\n",
      "     |      \n",
      "     |          >>> from prefect import flow\n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     my_task.submit()\n",
      "     |      \n",
      "     |          Wait for a task to finish\n",
      "     |      \n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     my_task.submit().wait()\n",
      "     |      \n",
      "     |          Use the result from a task in a flow\n",
      "     |      \n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     print(my_task.submit().result())\n",
      "     |          >>>\n",
      "     |          >>> my_flow()\n",
      "     |          hello\n",
      "     |      \n",
      "     |          Run an async task in an async flow\n",
      "     |      \n",
      "     |          >>> @task\n",
      "     |          >>> async def my_async_task():\n",
      "     |          >>>     pass\n",
      "     |          >>>\n",
      "     |          >>> @flow\n",
      "     |          >>> async def my_flow():\n",
      "     |          >>>     await my_async_task.submit()\n",
      "     |      \n",
      "     |          Run a sync task in an async flow\n",
      "     |      \n",
      "     |          >>> @flow\n",
      "     |          >>> async def my_flow():\n",
      "     |          >>>     my_task.submit()\n",
      "     |      \n",
      "     |          Enforce ordering between tasks that do not exchange data\n",
      "     |          >>> @task\n",
      "     |          >>> def task_1():\n",
      "     |          >>>     pass\n",
      "     |          >>>\n",
      "     |          >>> @task\n",
      "     |          >>> def task_2():\n",
      "     |          >>>     pass\n",
      "     |          >>>\n",
      "     |          >>> @flow\n",
      "     |          >>> def my_flow():\n",
      "     |          >>>     x = task_1.submit()\n",
      "     |          >>>\n",
      "     |          >>>     # task 2 will wait for task_1 to complete\n",
      "     |          >>>     y = task_2.submit(wait_for=[x])\n",
      "     |  \n",
      "     |  with_options(self, *, name: str = None, description: str = None, tags: Iterable[str] = None, cache_key_fn: Callable[[ForwardRef('TaskRunContext'), Dict[str, Any]], Optional[str]] = None, task_run_name: Union[Callable[[], str], str, NoneType] = None, cache_expiration: datetime.timedelta = None, retries: Optional[int] = <class 'prefect.utilities.annotations.NotSet'>, retry_delay_seconds: Union[float, int, List[float], Callable[[int], List[float]]] = <class 'prefect.utilities.annotations.NotSet'>, retry_jitter_factor: Optional[float] = <class 'prefect.utilities.annotations.NotSet'>, persist_result: Optional[bool] = <class 'prefect.utilities.annotations.NotSet'>, result_storage: Union[prefect.filesystems.WritableFileSystem, str, NoneType] = <class 'prefect.utilities.annotations.NotSet'>, result_serializer: Union[prefect.serializers.Serializer, str, NoneType] = <class 'prefect.utilities.annotations.NotSet'>, result_storage_key: Optional[str] = <class 'prefect.utilities.annotations.NotSet'>, cache_result_in_memory: Optional[bool] = None, timeout_seconds: Union[int, float] = None, log_prints: Optional[bool] = <class 'prefect.utilities.annotations.NotSet'>, refresh_cache: Optional[bool] = <class 'prefect.utilities.annotations.NotSet'>, on_completion: Optional[List[Callable[[ForwardRef('Task'), prefect.client.schemas.objects.TaskRun, prefect.client.schemas.objects.State], NoneType]]] = None, on_failure: Optional[List[Callable[[ForwardRef('Task'), prefect.client.schemas.objects.TaskRun, prefect.client.schemas.objects.State], NoneType]]] = None, viz_return_value: Optional[Any] = None)\n",
      "     |      Create a new task from the current object, updating provided options.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name: A new name for the task.\n",
      "     |          description: A new description for the task.\n",
      "     |          tags: A new set of tags for the task. If given, existing tags are ignored,\n",
      "     |              not merged.\n",
      "     |          cache_key_fn: A new cache key function for the task.\n",
      "     |          cache_expiration: A new cache expiration time for the task.\n",
      "     |          task_run_name: An optional name to distinguish runs of this task; this name can be provided\n",
      "     |              as a string template with the task's keyword arguments as variables,\n",
      "     |              or a function that returns a string.\n",
      "     |          retries: A new number of times to retry on task run failure.\n",
      "     |          retry_delay_seconds: Optionally configures how long to wait before retrying\n",
      "     |              the task after failure. This is only applicable if `retries` is nonzero.\n",
      "     |              This setting can either be a number of seconds, a list of retry delays,\n",
      "     |              or a callable that, given the total number of retries, generates a list\n",
      "     |              of retry delays. If a number of seconds, that delay will be applied to\n",
      "     |              all retries. If a list, each retry will wait for the corresponding delay\n",
      "     |              before retrying. When passing a callable or a list, the number of\n",
      "     |              configured retry delays cannot exceed 50.\n",
      "     |          retry_jitter_factor: An optional factor that defines the factor to which a\n",
      "     |              retry can be jittered in order to avoid a \"thundering herd\".\n",
      "     |          persist_result: A new option for enabling or disabling result persistence.\n",
      "     |          result_storage: A new storage type to use for results.\n",
      "     |          result_serializer: A new serializer to use for results.\n",
      "     |          result_storage_key: A new key for the persisted result to be stored at.\n",
      "     |          timeout_seconds: A new maximum time for the task to complete in seconds.\n",
      "     |          log_prints: A new option for enabling or disabling redirection of `print` statements.\n",
      "     |          refresh_cache: A new option for enabling or disabling cache refresh.\n",
      "     |          on_completion: A new list of callables to run when the task enters a completed state.\n",
      "     |          on_failure: A new list of callables to run when the task enters a failed state.\n",
      "     |          viz_return_value: An optional value to return when the task dependency tree is visualized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A new `Task` instance.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |          Create a new task from an existing task and update the name\n",
      "     |      \n",
      "     |          >>> @task(name=\"My task\")\n",
      "     |          >>> def my_task():\n",
      "     |          >>>     return 1\n",
      "     |          >>>\n",
      "     |          >>> new_task = my_task.with_options(name=\"My new task\")\n",
      "     |      \n",
      "     |          Create a new task from an existing task and update the retry settings\n",
      "     |      \n",
      "     |          >>> from random import randint\n",
      "     |          >>>\n",
      "     |          >>> @task(retries=1, retry_delay_seconds=5)\n",
      "     |          >>> def my_task():\n",
      "     |          >>>     x = randint(0, 5)\n",
      "     |          >>>     if x >= 3:  # Make a task that fails sometimes\n",
      "     |          >>>         raise ValueError(\"Retry me please!\")\n",
      "     |          >>>     return x\n",
      "     |          >>>\n",
      "     |          >>> new_task = my_task.with_options(retries=5, retry_delay_seconds=2)\n",
      "     |      \n",
      "     |          Use a task with updated options within a flow\n",
      "     |      \n",
      "     |          >>> @task(name=\"My task\")\n",
      "     |          >>> def my_task():\n",
      "     |          >>>     return 1\n",
      "     |          >>>\n",
      "     |          >>> @flow\n",
      "     |          >>> my_flow():\n",
      "     |          >>>     new_task = my_task.with_options(name=\"My new task\")\n",
      "     |          >>>     new_task()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[~P, ~R],)\n",
      "     |  \n",
      "     |  __parameters__ = (~P, ~R)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class allow_failure(BaseAnnotation)\n",
      "     |  allow_failure(value)\n",
      "     |  \n",
      "     |  Wrapper for states or futures.\n",
      "     |  \n",
      "     |  Indicates that the upstream run for this input can be failed.\n",
      "     |  \n",
      "     |  Generally, Prefect will not allow a downstream run to start if any of its inputs\n",
      "     |  are failed. This annotation allows you to opt into receiving a failed input\n",
      "     |  downstream.\n",
      "     |  \n",
      "     |  If the input is from a failed run, the attached exception will be passed to your\n",
      "     |  function.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      allow_failure\n",
      "     |      BaseAnnotation\n",
      "     |      BaseAnnotation\n",
      "     |      builtins.tuple\n",
      "     |      abc.ABC\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __orig_bases__ = (prefect.utilities.annotations.BaseAnnotation[~T],)\n",
      "     |  \n",
      "     |  __parameters__ = (~T,)\n",
      "     |  \n",
      "     |  _abc_impl = <_abc._abc_data object>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  __eq__(self, other: object) -> bool\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return a nicely formatted representation string\n",
      "     |  \n",
      "     |  rewrap(self, value: ~T) -> 'BaseAnnotation[T]'\n",
      "     |  \n",
      "     |  unwrap(self) -> ~T\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  __getnewargs__(self)\n",
      "     |      Return self as a plain tuple.  Used by copy and pickle.\n",
      "     |  \n",
      "     |  _asdict(self)\n",
      "     |      Return a new dict which maps field names to their values.\n",
      "     |  \n",
      "     |  _replace(self, /, **kwds)\n",
      "     |      Return a new BaseAnnotation object replacing specified fields with new values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  _make(iterable) from abc.ABCMeta\n",
      "     |      Make a new BaseAnnotation object from a sequence or iterable\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  __new__(_cls, value)\n",
      "     |      Create new instance of BaseAnnotation(value,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  value\n",
      "     |      Alias for field number 0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  __match_args__ = ('value',)\n",
      "     |  \n",
      "     |  _field_defaults = {}\n",
      "     |  \n",
      "     |  _fields = ('value',)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  count(self, value, /)\n",
      "     |      Return number of occurrences of value.\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      "     |      Return first index of value.\n",
      "     |      \n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from abc.ABCMeta\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from typing.Generic:\n",
      "     |  \n",
      "     |  _is_protocol = False\n",
      "    \n",
      "    class unmapped(BaseAnnotation)\n",
      "     |  unmapped(value)\n",
      "     |  \n",
      "     |  Wrapper for iterables.\n",
      "     |  \n",
      "     |  Indicates that this input should be sent as-is to all runs created during a mapping\n",
      "     |  operation instead of being split.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      unmapped\n",
      "     |      BaseAnnotation\n",
      "     |      BaseAnnotation\n",
      "     |      builtins.tuple\n",
      "     |      abc.ABC\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, _) -> ~T\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __orig_bases__ = (prefect.utilities.annotations.BaseAnnotation[~T],)\n",
      "     |  \n",
      "     |  __parameters__ = (~T,)\n",
      "     |  \n",
      "     |  _abc_impl = <_abc._abc_data object>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  __eq__(self, other: object) -> bool\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return a nicely formatted representation string\n",
      "     |  \n",
      "     |  rewrap(self, value: ~T) -> 'BaseAnnotation[T]'\n",
      "     |  \n",
      "     |  unwrap(self) -> ~T\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  __getnewargs__(self)\n",
      "     |      Return self as a plain tuple.  Used by copy and pickle.\n",
      "     |  \n",
      "     |  _asdict(self)\n",
      "     |      Return a new dict which maps field names to their values.\n",
      "     |  \n",
      "     |  _replace(self, /, **kwds)\n",
      "     |      Return a new BaseAnnotation object replacing specified fields with new values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  _make(iterable) from abc.ABCMeta\n",
      "     |      Make a new BaseAnnotation object from a sequence or iterable\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  __new__(_cls, value)\n",
      "     |      Create new instance of BaseAnnotation(value,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  value\n",
      "     |      Alias for field number 0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseAnnotation:\n",
      "     |  \n",
      "     |  __match_args__ = ('value',)\n",
      "     |  \n",
      "     |  _field_defaults = {}\n",
      "     |  \n",
      "     |  _fields = ('value',)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  count(self, value, /)\n",
      "     |      Return number of occurrences of value.\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      "     |      Return first index of value.\n",
      "     |      \n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from abc.ABCMeta\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from typing.Generic:\n",
      "     |  \n",
      "     |  _is_protocol = False\n",
      "\n",
      "FUNCTIONS\n",
      "    deploy(*deployments: prefect.deployments.runner.RunnerDeployment, work_pool_name: Optional[str] = None, image: Union[str, prefect.deployments.runner.DeploymentImage, NoneType] = None, build: bool = True, push: bool = True, print_next_steps_message: bool = True) -> List[uuid.UUID]\n",
      "        Deploy the provided list of deployments to dynamic infrastructure via a\n",
      "        work pool.\n",
      "        \n",
      "        By default, calling this function will build a Docker image for the deployments, push it to a\n",
      "        registry, and create each deployment via the Prefect API that will run the corresponding\n",
      "        flow on the given schedule.\n",
      "        \n",
      "        If you want to use an existing image, you can pass `build=False` to skip building and pushing\n",
      "        an image.\n",
      "        \n",
      "        Args:\n",
      "            *deployments: A list of deployments to deploy.\n",
      "            work_pool_name: The name of the work pool to use for these deployments. Defaults to\n",
      "                the value of `PREFECT_DEFAULT_WORK_POOL_NAME`.\n",
      "            image: The name of the Docker image to build, including the registry and\n",
      "                repository. Pass a DeploymentImage instance to customize the Dockerfile used\n",
      "                and build arguments.\n",
      "            build: Whether or not to build a new image for the flow. If False, the provided\n",
      "                image will be used as-is and pulled at runtime.\n",
      "            push: Whether or not to skip pushing the built image to a registry.\n",
      "            print_next_steps_message: Whether or not to print a message with next steps\n",
      "                after deploying the deployments.\n",
      "        \n",
      "        Returns:\n",
      "            A list of deployment IDs for the created/updated deployments.\n",
      "        \n",
      "        Examples:\n",
      "            Deploy a group of flows to a work pool:\n",
      "        \n",
      "            ```python\n",
      "            from prefect import deploy, flow\n",
      "        \n",
      "            @flow(log_prints=True)\n",
      "            def local_flow():\n",
      "                print(\"I'm a locally defined flow!\")\n",
      "        \n",
      "            if __name__ == \"__main__\":\n",
      "                deploy(\n",
      "                    local_flow.to_deployment(name=\"example-deploy-local-flow\"),\n",
      "                    flow.from_source(\n",
      "                        source=\"https://github.com/org/repo.git\",\n",
      "                        entrypoint=\"flows.py:my_flow\",\n",
      "                    ).to_deployment(\n",
      "                        name=\"example-deploy-remote-flow\",\n",
      "                    ),\n",
      "                    work_pool_name=\"my-work-pool\",\n",
      "                    image=\"my-registry/my-image:dev\",\n",
      "                )\n",
      "            ```\n",
      "    \n",
      "    flow(__fn=None, *, name: Optional[str] = None, version: Optional[str] = None, flow_run_name: Union[Callable[[], str], str, NoneType] = None, retries: int = None, retry_delay_seconds: Union[int, float] = None, task_runner: prefect.task_runners.BaseTaskRunner = <class 'prefect.task_runners.ConcurrentTaskRunner'>, description: str = None, timeout_seconds: Union[int, float] = None, validate_parameters: bool = True, persist_result: Optional[bool] = None, result_storage: Union[prefect.filesystems.WritableFileSystem, str, NoneType] = None, result_serializer: Union[prefect.serializers.Serializer, str, NoneType] = None, cache_result_in_memory: bool = True, log_prints: Optional[bool] = None, on_completion: Optional[List[Callable[[prefect.client.schemas.objects.Flow, prefect.client.schemas.objects.FlowRun, prefect.client.schemas.objects.State], NoneType]]] = None, on_failure: Optional[List[Callable[[prefect.client.schemas.objects.Flow, prefect.client.schemas.objects.FlowRun, prefect.client.schemas.objects.State], NoneType]]] = None, on_cancellation: Optional[List[Callable[[prefect.client.schemas.objects.Flow, prefect.client.schemas.objects.FlowRun, prefect.client.schemas.objects.State], NoneType]]] = None, on_crashed: Optional[List[Callable[[prefect.client.schemas.objects.Flow, prefect.client.schemas.objects.FlowRun, prefect.client.schemas.objects.State], NoneType]]] = None)\n",
      "        Decorator to designate a function as a Prefect workflow.\n",
      "        \n",
      "        This decorator may be used for asynchronous or synchronous functions.\n",
      "        \n",
      "        Flow parameters must be serializable by Pydantic.\n",
      "        \n",
      "        Args:\n",
      "            name: An optional name for the flow; if not provided, the name will be inferred\n",
      "                from the given function.\n",
      "            version: An optional version string for the flow; if not provided, we will\n",
      "                attempt to create a version string as a hash of the file containing the\n",
      "                wrapped function; if the file cannot be located, the version will be null.\n",
      "            flow_run_name: An optional name to distinguish runs of this flow; this name can\n",
      "                be provided as a string template with the flow's parameters as variables,\n",
      "                or a function that returns a string.\n",
      "            task_runner: An optional task runner to use for task execution within the flow; if\n",
      "                not provided, a `ConcurrentTaskRunner` will be instantiated.\n",
      "            description: An optional string description for the flow; if not provided, the\n",
      "                description will be pulled from the docstring for the decorated function.\n",
      "            timeout_seconds: An optional number of seconds indicating a maximum runtime for\n",
      "                the flow. If the flow exceeds this runtime, it will be marked as failed.\n",
      "                Flow execution may continue until the next task is called.\n",
      "            validate_parameters: By default, parameters passed to flows are validated by\n",
      "                Pydantic. This will check that input values conform to the annotated types\n",
      "                on the function. Where possible, values will be coerced into the correct\n",
      "                type; for example, if a parameter is defined as `x: int` and \"5\" is passed,\n",
      "                it will be resolved to `5`. If set to `False`, no validation will be\n",
      "                performed on flow parameters.\n",
      "            retries: An optional number of times to retry on flow run failure.\n",
      "            retry_delay_seconds: An optional number of seconds to wait before retrying the\n",
      "                flow after failure. This is only applicable if `retries` is nonzero.\n",
      "            persist_result: An optional toggle indicating whether the result of this flow\n",
      "                should be persisted to result storage. Defaults to `None`, which indicates\n",
      "                that Prefect should choose whether the result should be persisted depending on\n",
      "                the features being used.\n",
      "            result_storage: An optional block to use to persist the result of this flow.\n",
      "                This value will be used as the default for any tasks in this flow.\n",
      "                If not provided, the local file system will be used unless called as\n",
      "                a subflow, at which point the default will be loaded from the parent flow.\n",
      "            result_serializer: An optional serializer to use to serialize the result of this\n",
      "                flow for persistence. This value will be used as the default for any tasks\n",
      "                in this flow. If not provided, the value of `PREFECT_RESULTS_DEFAULT_SERIALIZER`\n",
      "                will be used unless called as a subflow, at which point the default will be\n",
      "                loaded from the parent flow.\n",
      "            log_prints: If set, `print` statements in the flow will be redirected to the\n",
      "                Prefect logger for the flow run. Defaults to `None`, which indicates that\n",
      "                the value from the parent flow should be used. If this is a parent flow,\n",
      "                the default is pulled from the `PREFECT_LOGGING_LOG_PRINTS` setting.\n",
      "            on_completion: An optional list of functions to call when the flow run is\n",
      "                completed. Each function should accept three arguments: the flow, the flow\n",
      "                run, and the final state of the flow run.\n",
      "            on_failure: An optional list of functions to call when the flow run fails. Each\n",
      "                function should accept three arguments: the flow, the flow run, and the\n",
      "                final state of the flow run.\n",
      "            on_cancellation: An optional list of functions to call when the flow run is\n",
      "                cancelled. These functions will be passed the flow, flow run, and final state.\n",
      "            on_crashed: An optional list of functions to call when the flow run crashes. Each\n",
      "                function should accept three arguments: the flow, the flow run, and the\n",
      "                final state of the flow run.\n",
      "        \n",
      "        Returns:\n",
      "            A callable `Flow` object which, when called, will run the flow and return its\n",
      "            final state.\n",
      "        \n",
      "        Examples:\n",
      "            Define a simple flow\n",
      "        \n",
      "            >>> from prefect import flow\n",
      "            >>> @flow\n",
      "            >>> def add(x, y):\n",
      "            >>>     return x + y\n",
      "        \n",
      "            Define an async flow\n",
      "        \n",
      "            >>> @flow\n",
      "            >>> async def add(x, y):\n",
      "            >>>     return x + y\n",
      "        \n",
      "            Define a flow with a version and description\n",
      "        \n",
      "            >>> @flow(version=\"first-flow\", description=\"This flow is empty!\")\n",
      "            >>> def my_flow():\n",
      "            >>>     pass\n",
      "        \n",
      "            Define a flow with a custom name\n",
      "        \n",
      "            >>> @flow(name=\"The Ultimate Flow\")\n",
      "            >>> def my_flow():\n",
      "            >>>     pass\n",
      "        \n",
      "            Define a flow that submits its tasks to dask\n",
      "        \n",
      "            >>> from prefect_dask.task_runners import DaskTaskRunner\n",
      "            >>>\n",
      "            >>> @flow(task_runner=DaskTaskRunner)\n",
      "            >>> def my_flow():\n",
      "            >>>     pass\n",
      "    \n",
      "    get_client(httpx_settings: Optional[dict] = None) -> 'PrefectClient'\n",
      "        Retrieve a HTTP client for communicating with the Prefect REST API.\n",
      "        \n",
      "        The client must be context managed; for example:\n",
      "        \n",
      "        ```python\n",
      "        async with get_client() as client:\n",
      "            await client.hello()\n",
      "        ```\n",
      "    \n",
      "    get_run_logger(context: 'RunContext' = None, **kwargs: str) -> Union[logging.Logger, logging.LoggerAdapter]\n",
      "        Get a Prefect logger for the current task run or flow run.\n",
      "        \n",
      "        The logger will be named either `prefect.task_runs` or `prefect.flow_runs`.\n",
      "        Contextual data about the run will be attached to the log records.\n",
      "        \n",
      "        These loggers are connected to the `APILogHandler` by default to send log records to\n",
      "        the API.\n",
      "        \n",
      "        Arguments:\n",
      "            context: A specific context may be provided as an override. By default, the\n",
      "                context is inferred from global state and this should not be needed.\n",
      "            **kwargs: Additional keyword arguments will be attached to the log records in\n",
      "                addition to the run metadata\n",
      "        \n",
      "        Raises:\n",
      "            RuntimeError: If no context can be found\n",
      "    \n",
      "    serve(*args: prefect.deployments.runner.RunnerDeployment, pause_on_shutdown: bool = True, print_starting_message: bool = True, **kwargs)\n",
      "        Serve the provided list of deployments.\n",
      "        \n",
      "        Args:\n",
      "            *args: A list of deployments to serve.\n",
      "            pause_on_shutdown: A boolean for whether or not to automatically pause\n",
      "                deployment schedules on shutdown.\n",
      "            **kwargs: Additional keyword arguments to pass to the runner.\n",
      "        \n",
      "        Examples:\n",
      "            Prepare two deployments and serve them:\n",
      "        \n",
      "            ```python\n",
      "            import datetime\n",
      "        \n",
      "            from prefect import flow, serve\n",
      "        \n",
      "            @flow\n",
      "            def my_flow(name):\n",
      "                print(f\"hello {name}\")\n",
      "        \n",
      "            @flow\n",
      "            def my_other_flow(name):\n",
      "                print(f\"goodbye {name}\")\n",
      "        \n",
      "            if __name__ == \"__main__\":\n",
      "                # Run once a day\n",
      "                hello_deploy = my_flow.to_deployment(\n",
      "                    \"hello\", tags=[\"dev\"], interval=datetime.timedelta(days=1)\n",
      "                )\n",
      "        \n",
      "                # Run every Sunday at 4:00 AM\n",
      "                bye_deploy = my_other_flow.to_deployment(\n",
      "                    \"goodbye\", tags=[\"dev\"], cron=\"0 4 * * sun\"\n",
      "                )\n",
      "        \n",
      "                serve(hello_deploy, bye_deploy)\n",
      "            ```\n",
      "    \n",
      "    tags(*new_tags: str) -> Set[str]\n",
      "        Context manager to add tags to flow and task run calls.\n",
      "        \n",
      "        Tags are always combined with any existing tags.\n",
      "        \n",
      "        Yields:\n",
      "            The current set of tags\n",
      "        \n",
      "        Examples:\n",
      "            >>> from prefect import tags, task, flow\n",
      "            >>> @task\n",
      "            >>> def my_task():\n",
      "            >>>     pass\n",
      "        \n",
      "            Run a task with tags\n",
      "        \n",
      "            >>> @flow\n",
      "            >>> def my_flow():\n",
      "            >>>     with tags(\"a\", \"b\"):\n",
      "            >>>         my_task()  # has tags: a, b\n",
      "        \n",
      "            Run a flow with tags\n",
      "        \n",
      "            >>> @flow\n",
      "            >>> def my_flow():\n",
      "            >>>     pass\n",
      "            >>> with tags(\"a\", \"b\"):\n",
      "            >>>     my_flow()  # has tags: a, b\n",
      "        \n",
      "            Run a task with nested tag contexts\n",
      "        \n",
      "            >>> @flow\n",
      "            >>> def my_flow():\n",
      "            >>>     with tags(\"a\", \"b\"):\n",
      "            >>>         with tags(\"c\", \"d\"):\n",
      "            >>>             my_task()  # has tags: a, b, c, d\n",
      "            >>>         my_task()  # has tags: a, b\n",
      "        \n",
      "            Inspect the current tags\n",
      "        \n",
      "            >>> @flow\n",
      "            >>> def my_flow():\n",
      "            >>>     with tags(\"c\", \"d\"):\n",
      "            >>>         with tags(\"e\", \"f\") as current_tags:\n",
      "            >>>              print(current_tags)\n",
      "            >>> with tags(\"a\", \"b\"):\n",
      "            >>>     my_flow()\n",
      "            {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n",
      "    \n",
      "    task(__fn=None, *, name: str = None, description: str = None, tags: Iterable[str] = None, version: str = None, cache_key_fn: Callable[[ForwardRef('TaskRunContext'), Dict[str, Any]], Optional[str]] = None, cache_expiration: datetime.timedelta = None, task_run_name: Union[Callable[[], str], str, NoneType] = None, retries: int = None, retry_delay_seconds: Union[float, int, List[float], Callable[[int], List[float]]] = None, retry_jitter_factor: Optional[float] = None, persist_result: Optional[bool] = None, result_storage: Union[prefect.filesystems.WritableFileSystem, str, NoneType] = None, result_storage_key: Optional[str] = None, result_serializer: Union[prefect.serializers.Serializer, str, NoneType] = None, cache_result_in_memory: bool = True, timeout_seconds: Union[int, float] = None, log_prints: Optional[bool] = None, refresh_cache: Optional[bool] = None, on_completion: Optional[List[Callable[[ForwardRef('Task'), prefect.client.schemas.objects.TaskRun, prefect.client.schemas.objects.State], NoneType]]] = None, on_failure: Optional[List[Callable[[ForwardRef('Task'), prefect.client.schemas.objects.TaskRun, prefect.client.schemas.objects.State], NoneType]]] = None, viz_return_value: Any = None)\n",
      "        Decorator to designate a function as a task in a Prefect workflow.\n",
      "        \n",
      "        This decorator may be used for asynchronous or synchronous functions.\n",
      "        \n",
      "        Args:\n",
      "            name: An optional name for the task; if not provided, the name will be inferred\n",
      "                from the given function.\n",
      "            description: An optional string description for the task.\n",
      "            tags: An optional set of tags to be associated with runs of this task. These\n",
      "                tags are combined with any tags defined by a `prefect.tags` context at\n",
      "                task runtime.\n",
      "            version: An optional string specifying the version of this task definition\n",
      "            cache_key_fn: An optional callable that, given the task run context and call\n",
      "                parameters, generates a string key; if the key matches a previous completed\n",
      "                state, that state result will be restored instead of running the task again.\n",
      "            cache_expiration: An optional amount of time indicating how long cached states\n",
      "                for this task should be restorable; if not provided, cached states will\n",
      "                never expire.\n",
      "            task_run_name: An optional name to distinguish runs of this task; this name can be provided\n",
      "                as a string template with the task's keyword arguments as variables,\n",
      "                or a function that returns a string.\n",
      "            retries: An optional number of times to retry on task run failure\n",
      "            retry_delay_seconds: Optionally configures how long to wait before retrying the\n",
      "                task after failure. This is only applicable if `retries` is nonzero. This\n",
      "                setting can either be a number of seconds, a list of retry delays, or a\n",
      "                callable that, given the total number of retries, generates a list of retry\n",
      "                delays. If a number of seconds, that delay will be applied to all retries.\n",
      "                If a list, each retry will wait for the corresponding delay before retrying.\n",
      "                When passing a callable or a list, the number of configured retry delays\n",
      "                cannot exceed 50.\n",
      "            retry_jitter_factor: An optional factor that defines the factor to which a retry\n",
      "                can be jittered in order to avoid a \"thundering herd\".\n",
      "            persist_result: An optional toggle indicating whether the result of this task\n",
      "                should be persisted to result storage. Defaults to `None`, which indicates\n",
      "                that Prefect should choose whether the result should be persisted depending on\n",
      "                the features being used.\n",
      "            result_storage: An optional block to use to persist the result of this task.\n",
      "                Defaults to the value set in the flow the task is called in.\n",
      "            result_storage_key: An optional key to store the result in storage at when persisted.\n",
      "                Defaults to a unique identifier.\n",
      "            result_serializer: An optional serializer to use to serialize the result of this\n",
      "                task for persistence. Defaults to the value set in the flow the task is\n",
      "                called in.\n",
      "            timeout_seconds: An optional number of seconds indicating a maximum runtime for\n",
      "                the task. If the task exceeds this runtime, it will be marked as failed.\n",
      "            log_prints: If set, `print` statements in the task will be redirected to the\n",
      "                Prefect logger for the task run. Defaults to `None`, which indicates\n",
      "                that the value from the flow should be used.\n",
      "            refresh_cache: If set, cached results for the cache key are not used.\n",
      "                Defaults to `None`, which indicates that a cached result from a previous\n",
      "                execution with matching cache key is used.\n",
      "            on_failure: An optional list of callables to run when the task enters a failed state.\n",
      "            on_completion: An optional list of callables to run when the task enters a completed state.\n",
      "            viz_return_value: An optional value to return when the task dependency tree is visualized.\n",
      "        \n",
      "        Returns:\n",
      "            A callable `Task` object which, when called, will submit the task for execution.\n",
      "        \n",
      "        Examples:\n",
      "            Define a simple task\n",
      "        \n",
      "            >>> @task\n",
      "            >>> def add(x, y):\n",
      "            >>>     return x + y\n",
      "        \n",
      "            Define an async task\n",
      "        \n",
      "            >>> @task\n",
      "            >>> async def add(x, y):\n",
      "            >>>     return x + y\n",
      "        \n",
      "            Define a task with tags and a description\n",
      "        \n",
      "            >>> @task(tags={\"a\", \"b\"}, description=\"This task is empty but its my first!\")\n",
      "            >>> def my_task():\n",
      "            >>>     pass\n",
      "        \n",
      "            Define a task with a custom name\n",
      "        \n",
      "            >>> @task(name=\"The Ultimate Task\")\n",
      "            >>> def my_task():\n",
      "            >>>     pass\n",
      "        \n",
      "            Define a task that retries 3 times with a 5 second delay between attempts\n",
      "        \n",
      "            >>> from random import randint\n",
      "            >>>\n",
      "            >>> @task(retries=3, retry_delay_seconds=5)\n",
      "            >>> def my_task():\n",
      "            >>>     x = randint(0, 5)\n",
      "            >>>     if x >= 3:  # Make a task that fails sometimes\n",
      "            >>>         raise ValueError(\"Retry me please!\")\n",
      "            >>>     return x\n",
      "        \n",
      "            Define a task that is cached for a day based on its inputs\n",
      "        \n",
      "            >>> from prefect.tasks import task_input_hash\n",
      "            >>> from datetime import timedelta\n",
      "            >>>\n",
      "            >>> @task(cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\n",
      "            >>> def my_task():\n",
      "            >>>     return \"hello\"\n",
      "\n",
      "DATA\n",
      "    __all__ = ['allow_failure', 'flow', 'Flow', 'get_client', 'get_run_log...\n",
      "    __development_base_path__ = PosixPath('/home/c99/anaconda3/lib/python3...\n",
      "    __module_path__ = PosixPath('/home/c99/anaconda3/lib/python3.10/site-p...\n",
      "    __ui_static_path__ = PosixPath('/home/c99/anaconda3/lib/python3.10/sit...\n",
      "    __version_info__ = {'date': '2023-11-22T15:19:40-0600', 'dirty': False...\n",
      "\n",
      "VERSION\n",
      "    v2.14.6\n",
      "\n",
      "FILE\n",
      "    /home/c99/anaconda3/lib/python3.10/site-packages/prefect/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# help(prefect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefect \n",
    "\n",
    "Prefect is an orchestration an observability platform that enables developers to build and scale resilient code quickly, turning Python scripts to resilient, reccurring workkflows.\n",
    "\n",
    "What does that mean. Ideally we want our pipelines to be robust and gracefully recover from errors. For instance, while attempting to ingest data from a data source, the data source may not be available, in that case, we want to make our pipeline *resilient* by retrying the ingestion phase.  \n",
    "\n",
    "We will use an example from our previous pipelines to demonstrate to use Prefect for workflow orchestration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_all_tickers():\n",
    "  BASE_URL = \"https://api.polygon.io/v3/reference/tickers?\"\n",
    "  params = {\"apiKey\" : \"cmDK3EffgqLXrbZ0ZivQ9I7ZAwjHImiX\"}\n",
    "\n",
    "  print(f\"Beginning data extraction from {BASE_URL}\")\n",
    "  try:\n",
    "    res = requests.get(BASE_URL, params=params)\n",
    "    data = res.json()[\"results\"]\n",
    "    df = pd.DataFrame(data)\n",
    "  except Exception as e:\n",
    "    print(f\"Error {e} while ingesting data from {BASE_URL}\")\n",
    "    df = pd.DataFrame()   # return empty dataframe if exception is raised\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our function to consume all the companies information supported by Polygon API. This is a good candidate for the dimension table. Before loading the data, we must transform the data, then use Prefect to orchestrate the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning data extraction from https://api.polygon.io/v3/reference/tickers?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>name</th>\n",
       "      <th>market</th>\n",
       "      <th>locale</th>\n",
       "      <th>primary_exchange</th>\n",
       "      <th>type</th>\n",
       "      <th>active</th>\n",
       "      <th>currency_name</th>\n",
       "      <th>cik</th>\n",
       "      <th>composite_figi</th>\n",
       "      <th>share_class_figi</th>\n",
       "      <th>last_updated_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>AAT</td>\n",
       "      <td>AMERICAN ASSETS TRUST, INC.</td>\n",
       "      <td>stocks</td>\n",
       "      <td>us</td>\n",
       "      <td>XNYS</td>\n",
       "      <td>CS</td>\n",
       "      <td>True</td>\n",
       "      <td>usd</td>\n",
       "      <td>0001500217</td>\n",
       "      <td>BBG00161BCR0</td>\n",
       "      <td>BBG001TCBJS5</td>\n",
       "      <td>2024-02-28T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>ABCFF</td>\n",
       "      <td>ABACUS MINING &amp; EXPL ORD</td>\n",
       "      <td>otc</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>True</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BBG000JR8L77</td>\n",
       "      <td>BBG001S5XSV3</td>\n",
       "      <td>2023-05-10T05:00:38.805Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>AAPI</td>\n",
       "      <td>APPLE ISPORTS GROUP INC</td>\n",
       "      <td>otc</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CS</td>\n",
       "      <td>True</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BBG000CQN9X7</td>\n",
       "      <td>BBG001SGHPN2</td>\n",
       "      <td>2024-02-21T09:45:13.158Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>ABBNY</td>\n",
       "      <td>ABB LTD SPONS ADR</td>\n",
       "      <td>otc</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADRC</td>\n",
       "      <td>True</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BBG000DK5Q25</td>\n",
       "      <td>BBG001SDDMX9</td>\n",
       "      <td>2023-06-30T05:00:38.266Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AAGRW</td>\n",
       "      <td>African Agriculture Holdings Inc. Warrant</td>\n",
       "      <td>stocks</td>\n",
       "      <td>us</td>\n",
       "      <td>XNAS</td>\n",
       "      <td>WARRANT</td>\n",
       "      <td>True</td>\n",
       "      <td>usd</td>\n",
       "      <td>0001848898</td>\n",
       "      <td>BBG00ZKXGR82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-28T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AAGC</td>\n",
       "      <td>ALL AMERICAN GOLD CORP</td>\n",
       "      <td>otc</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CS</td>\n",
       "      <td>True</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BBG000R4MM71</td>\n",
       "      <td>BBG001T948W9</td>\n",
       "      <td>2024-02-23T09:45:11.986Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AACIU</td>\n",
       "      <td>Armada Acquisition Corp. I Unit</td>\n",
       "      <td>stocks</td>\n",
       "      <td>us</td>\n",
       "      <td>XNAS</td>\n",
       "      <td>UNIT</td>\n",
       "      <td>True</td>\n",
       "      <td>usd</td>\n",
       "      <td>0001844817</td>\n",
       "      <td>BBG011PFP1D1</td>\n",
       "      <td>BBG011PFP285</td>\n",
       "      <td>2024-02-28T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>AANNF</td>\n",
       "      <td>AROUNDTOWN SA ORD</td>\n",
       "      <td>otc</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>True</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-09-28T08:45:23.537Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>AAKAY</td>\n",
       "      <td>AAK AB UNSP/ADR</td>\n",
       "      <td>otc</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADRC</td>\n",
       "      <td>True</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BBG01L562BD2</td>\n",
       "      <td>BBG01L562C77</td>\n",
       "      <td>2024-01-31T18:59:08.681Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAA</td>\n",
       "      <td>AXS First Priority CLO Bond ETF</td>\n",
       "      <td>stocks</td>\n",
       "      <td>us</td>\n",
       "      <td>ARCX</td>\n",
       "      <td>ETF</td>\n",
       "      <td>True</td>\n",
       "      <td>usd</td>\n",
       "      <td>0001776878</td>\n",
       "      <td>BBG01B0JRCS6</td>\n",
       "      <td>BBG01B0JRCT5</td>\n",
       "      <td>2024-02-28T00:00:00Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticker                                       name  market locale  \\\n",
       "61    AAT                AMERICAN ASSETS TRUST, INC.  stocks     us   \n",
       "92  ABCFF                   ABACUS MINING & EXPL ORD     otc     us   \n",
       "49   AAPI                    APPLE ISPORTS GROUP INC     otc     us   \n",
       "84  ABBNY                          ABB LTD SPONS ADR     otc     us   \n",
       "28  AAGRW  African Agriculture Holdings Inc. Warrant  stocks     us   \n",
       "23   AAGC                     ALL AMERICAN GOLD CORP     otc     us   \n",
       "12  AACIU            Armada Acquisition Corp. I Unit  stocks     us   \n",
       "43  AANNF                          AROUNDTOWN SA ORD     otc     us   \n",
       "34  AAKAY                            AAK AB UNSP/ADR     otc     us   \n",
       "2     AAA            AXS First Priority CLO Bond ETF  stocks     us   \n",
       "\n",
       "   primary_exchange     type  active currency_name         cik composite_figi  \\\n",
       "61             XNYS       CS    True           usd  0001500217   BBG00161BCR0   \n",
       "92              NaN       OS    True           USD         NaN   BBG000JR8L77   \n",
       "49              NaN       CS    True           USD         NaN   BBG000CQN9X7   \n",
       "84              NaN     ADRC    True           USD         NaN   BBG000DK5Q25   \n",
       "28             XNAS  WARRANT    True           usd  0001848898   BBG00ZKXGR82   \n",
       "23              NaN       CS    True           USD         NaN   BBG000R4MM71   \n",
       "12             XNAS     UNIT    True           usd  0001844817   BBG011PFP1D1   \n",
       "43              NaN       OS    True           USD         NaN            NaN   \n",
       "34              NaN     ADRC    True           USD         NaN   BBG01L562BD2   \n",
       "2              ARCX      ETF    True           usd  0001776878   BBG01B0JRCS6   \n",
       "\n",
       "   share_class_figi          last_updated_utc  \n",
       "61     BBG001TCBJS5      2024-02-28T00:00:00Z  \n",
       "92     BBG001S5XSV3  2023-05-10T05:00:38.805Z  \n",
       "49     BBG001SGHPN2  2024-02-21T09:45:13.158Z  \n",
       "84     BBG001SDDMX9  2023-06-30T05:00:38.266Z  \n",
       "28              NaN      2024-02-28T00:00:00Z  \n",
       "23     BBG001T948W9  2024-02-23T09:45:11.986Z  \n",
       "12     BBG011PFP285      2024-02-28T00:00:00Z  \n",
       "43              NaN  2021-09-28T08:45:23.537Z  \n",
       "34     BBG01L562C77  2024-01-31T18:59:08.681Z  \n",
       "2      BBG01B0JRCT5      2024-02-28T00:00:00Z  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def extract_all_tickers():\n",
    "  BASE_URL = \"https://api.polygon.io/v3/reference/tickers?\"\n",
    "  params = {\"apiKey\" : \"cmDK3EffgqLXrbZ0ZivQ9I7ZAwjHImiX\"}\n",
    "\n",
    "  print(f\"Beginning data extraction from {BASE_URL}\")\n",
    "  try:\n",
    "    res = requests.get(BASE_URL, params=params)\n",
    "    data = res.json()[\"results\"]\n",
    "    df = pd.DataFrame(data)\n",
    "  except Exception as e:\n",
    "    print(f\"Error {e} while ingesting data from {BASE_URL}\")\n",
    "    df = pd.DataFrame()   # return empty dataframe if exception is raised\n",
    "\n",
    "  return df\n",
    "\n",
    "dim_companies = extract_all_tickers()\n",
    "dim_companies.sample(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefect Concepts - Tasks and Flows\n",
    "\n",
    "**Tasks** are functions, they can take inputs, perform work, and return an output. In context of a data pipeline, the *extract*, *transform* and *load* functions would be Prefect tasks in a Prefect workflow. \n",
    "\n",
    "Think of a task as enhanced function. In our vanilla functions, we had to manually implement logging. A Prefect task has automatic logging to capture details such as runtime, tags, and final state.\n",
    "\n",
    "**Flows**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making our extract function into a task\n",
    "\n",
    "@task(name=\"Polygon finance ingestion\", log_prints=True, retries=3)\n",
    "def extract_all_tickers():\n",
    "  BASE_URL = \"https://api.polygon.io/v3/reference/tickers?\"\n",
    "  params = {\"apiKey\" : \"cmDK3EffgqLXrbZ0ZivQ9I7ZAwjHImiX\"}\n",
    "\n",
    "  print(f\"Beginning data extraction from {BASE_URL}\")\n",
    "  try:\n",
    "    res = requests.get(BASE_URL, params=params)\n",
    "    data = res.json()[\"results\"]\n",
    "    df = pd.DataFrame(data)\n",
    "  except Exception as e:\n",
    "    print(f\"Error {e} while ingesting data from {BASE_URL}\")\n",
    "    df = pd.DataFrame()   # return empty dataframe if exception is raised\n",
    "\n",
    "  return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
